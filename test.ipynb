{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54004f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76639f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_text = pd.read_csv('Fake.csv')\n",
    "real_text = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b789c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_text['label'] = 1\n",
    "real_text['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb1e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([fake_text, real_text]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371e0ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "44893  'Fully committed' NATO backs new U.S. approach...   \n",
       "44894  LexisNexis withdrew two products from Chinese ...   \n",
       "44895  Minsk cultural hub becomes haven from authorities   \n",
       "44896  Vatican upbeat on possibility of Pope Francis ...   \n",
       "44897  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "44894  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "44895  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "44896  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date  label  \n",
       "0      December 31, 2017      1  \n",
       "1      December 31, 2017      1  \n",
       "2      December 30, 2017      1  \n",
       "3      December 29, 2017      1  \n",
       "4      December 25, 2017      1  \n",
       "...                  ...    ...  \n",
       "44893   August 22, 2017       0  \n",
       "44894   August 22, 2017       0  \n",
       "44895   August 22, 2017       0  \n",
       "44896   August 22, 2017       0  \n",
       "44897   August 22, 2017       0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93908b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['title', 'subject', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7138a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Donald Trump just couldn t wish all Americans ...      1\n",
       "1      House Intelligence Committee Chairman Devin Nu...      1\n",
       "2      On Friday, it was revealed that former Milwauk...      1\n",
       "3      On Christmas day, Donald Trump announced that ...      1\n",
       "4      Pope Francis used his annual Christmas Day mes...      1\n",
       "...                                                  ...    ...\n",
       "44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...      0\n",
       "44894  LONDON (Reuters) - LexisNexis, a provider of l...      0\n",
       "44895  MINSK (Reuters) - In the shadow of disused Sov...      0\n",
       "44896  MOSCOW (Reuters) - Vatican Secretary of State ...      0\n",
       "44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...      0\n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae522266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to c:\\VSCode Codes\\Fake-\n",
      "[nltk_data]     News-Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to c:\\VSCode Codes\\Fake-News-\n",
      "[nltk_data]     Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to c:\\VSCode Codes\\Fake-News-\n",
      "[nltk_data]     Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to c:\\VSCode Codes\\Fake-\n",
      "[nltk_data]     News-Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# run \"mkdir -p fnd-venv/nltk_data\" in your venv terminal to create the directory for these 3 nltk data files\n",
    "nltk.data.path.append(\"fnd-venv/nltk_data\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # 1. Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove any special characters \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # 3. Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c883f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7651dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[donald, trump, wish, american, happy, new, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>1</td>\n",
       "      <td>[friday, revealed, former, milwaukee, sheriff,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[christmas, day, donald, trump, announced, wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>1</td>\n",
       "      <td>[pope, francis, used, annual, christmas, day, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>0</td>\n",
       "      <td>[brussels, reuters, nato, ally, tuesday, welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>0</td>\n",
       "      <td>[london, reuters, lexisnexis, provider, legal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>0</td>\n",
       "      <td>[minsk, reuters, shadow, disused, sovietera, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[moscow, reuters, vatican, secretary, state, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[jakarta, reuters, indonesia, buy, sukhoi, fig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...      1   \n",
       "1      House Intelligence Committee Chairman Devin Nu...      1   \n",
       "2      On Friday, it was revealed that former Milwauk...      1   \n",
       "3      On Christmas day, Donald Trump announced that ...      1   \n",
       "4      Pope Francis used his annual Christmas Day mes...      1   \n",
       "...                                                  ...    ...   \n",
       "44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...      0   \n",
       "44894  LONDON (Reuters) - LexisNexis, a provider of l...      0   \n",
       "44895  MINSK (Reuters) - In the shadow of disused Sov...      0   \n",
       "44896  MOSCOW (Reuters) - Vatican Secretary of State ...      0   \n",
       "44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...      0   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [donald, trump, wish, american, happy, new, ye...  \n",
       "1      [house, intelligence, committee, chairman, dev...  \n",
       "2      [friday, revealed, former, milwaukee, sheriff,...  \n",
       "3      [christmas, day, donald, trump, announced, wou...  \n",
       "4      [pope, francis, used, annual, christmas, day, ...  \n",
       "...                                                  ...  \n",
       "44893  [brussels, reuters, nato, ally, tuesday, welco...  \n",
       "44894  [london, reuters, lexisnexis, provider, legal,...  \n",
       "44895  [minsk, reuters, shadow, disused, sovietera, f...  \n",
       "44896  [moscow, reuters, vatican, secretary, state, c...  \n",
       "44897  [jakarta, reuters, indonesia, buy, sukhoi, fig...  \n",
       "\n",
       "[44898 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e025bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 111204\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "\n",
    "# Keep only tokens that appear >= 2 times\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(Counter(all_tokens).items()) if count >= 2}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2925c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "df['encoded'] = df['tokens'].apply(encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341f1714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index used after re-encoding: 203389\n",
      "Vocab size: 111204\n"
     ]
    }
   ],
   "source": [
    "max_index = max([max(seq) for seq in df['encoded'] if len(seq) > 0])\n",
    "print(\"Max index used after re-encoding:\", max_index)\n",
    "print(\"Vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2601594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences with out-of-range indices: 28659\n"
     ]
    }
   ],
   "source": [
    "bad_indices = [(i, max(seq)) for i, seq in enumerate(df['encoded']) if len(seq) > 0 and max(seq) > len(vocab)]\n",
    "print(\"Number of sequences with out-of-range indices:\", len(bad_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3f498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to c:\\VSCode Codes\\Fake-\n",
      "[nltk_data]     News-Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to c:\\VSCode Codes\\Fake-News-\n",
      "[nltk_data]     Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to c:\\VSCode Codes\\Fake-News-\n",
      "[nltk_data]     Detection-Project\\fnd-venv\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 111204\n",
      "Max index used after re-encoding: 203389\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Setup nltk\n",
    "nltk.data.path.append(\"fnd-venv/nltk_data\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function that returns tokens\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# STEP 3: Drop unused columns and preprocess\n",
    "df = df.drop(columns=['title', 'subject', 'date'])\n",
    "df['tokens'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# STEP 4: Build vocab ONLY from preprocessed tokens\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(Counter(all_tokens).items()) if count >= 2}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))  # should be in sync with actual token indices\n",
    "\n",
    "# STEP 5: Encode using vocab\n",
    "def encode(tokens):\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "df['encoded'] = df['tokens'].apply(encode)\n",
    "\n",
    "# STEP 6: Check for any out-of-range indices\n",
    "max_idx = max([max(seq) if seq else 0 for seq in df['encoded']])\n",
    "print(\"Max index used after re-encoding:\", max_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5587ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded example 0: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 4, 31, 32, 13, 14, 33, 15, 16, 17, 18, 6, 34, 7, 8, 35, 36, 37, 38, 39, 8, 40, 25, 26, 27, 28, 29, 30, 4, 31, 32, 13, 14, 33, 15, 16, 17, 18, 6, 34, 7, 8, 39, 8, 40, 2, 41, 3, 42, 43, 3, 44, 45, 1, 1, 48, 35, 49, 7, 8, 50, 51, 52, 53, 54, 55, 3, 56, 57, 33, 58, 59, 60, 61, 62, 4, 5, 63, 6, 7, 8, 64, 65, 66, 67, 43, 23, 51, 68, 69, 43, 70, 71, 72, 39, 8, 40, 73, 74, 75, 76, 77, 78, 79, 80, 43, 81, 82, 83, 84, 85, 86, 87, 86, 88, 89, 1, 43, 91, 92, 14, 7, 8, 4, 93, 93, 43, 94, 6, 7, 8, 1, 1, 1, 43, 3, 7, 8, 98, 44, 6, 7, 8, 99, 84, 13, 100, 101, 102, 103, 104, 2, 41, 3, 42, 43, 105, 7, 3, 106, 107, 108, 13, 14, 7, 8, 109, 110, 111, 1, 113, 114, 115, 43, 3, 116, 44, 117, 1, 61, 119, 120, 121, 35, 122, 1, 1, 43, 125, 51, 126, 127, 8, 128, 129, 130, 1, 1, 43, 133, 134, 91, 135, 14, 136, 1, 43, 138, 8, 139, 103, 140, 43, 85, 141, 142, 143, 71, 144, 145, 146, 147, 148, 149, 144, 8, 1, 151, 152, 153]\n",
      "Encoded example 1: [154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 51, 84, 164, 165, 1, 167, 168, 169, 170, 171, 172, 173, 174, 175, 3, 176, 177, 178, 169, 179, 180, 181, 7, 182, 183, 3, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 187, 199, 200, 3, 201, 202, 203, 204, 205, 206, 207, 208, 209, 189, 190, 210, 200, 211, 212, 7, 182, 213, 3, 35, 214, 215, 216, 217, 218, 219, 220, 221, 19, 222, 223, 213, 224, 225, 3, 226, 3, 227, 228, 210, 1, 230, 187, 191, 231, 232, 233, 234, 193, 235, 236, 237, 238, 239, 240, 241, 242, 196, 243, 244, 203, 245, 187, 246, 247, 248, 189, 249, 231, 233, 234, 250, 251, 252, 218, 219, 253, 254, 255, 256, 257, 258, 259, 231, 235, 260, 261, 245, 187, 5, 262, 179, 263, 264, 19, 5, 265, 235, 266, 192, 231, 205, 187, 267, 268, 269, 173, 270, 271, 272, 273, 274, 275, 276, 277, 102, 278, 20, 279, 1, 281, 282, 153]\n",
      "Encoded example 2: [283, 191, 19, 284, 285, 286, 287, 288, 289, 290, 291, 2, 3, 201, 256, 292, 293, 294, 295, 296, 297, 287, 298, 299, 300, 301, 253, 302, 303, 304, 305, 306, 307, 308, 309, 287, 310, 303, 311, 301, 1, 313, 314, 315, 173, 316, 1, 318, 16, 17, 33, 319, 320, 313, 314, 321, 322, 323, 18, 324, 325, 326, 16, 17, 251, 327, 328, 19, 285, 38, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 85, 342, 322, 323, 18, 324, 325, 326, 16, 17, 251, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 85, 342, 1, 286, 287, 344, 345, 43, 311, 1, 17, 269, 323, 18, 72, 16, 17, 325, 347, 348, 349, 350, 351, 72, 352, 353, 105, 339, 354, 51, 269, 323, 18, 355, 341, 11, 352, 353, 1, 1, 286, 287, 344, 345, 43, 321, 358, 359, 360, 361, 313, 314, 16, 362, 363, 364, 365, 366, 367, 329, 368, 219, 369, 364, 287, 256, 313, 314, 370, 1, 372, 372, 43, 373, 374, 375, 85, 376, 377, 378, 1, 43, 1, 381, 382, 383, 384, 1, 386, 387, 388, 43, 1, 390, 1, 1, 43, 358, 303, 31, 393, 394, 395, 396, 397, 398, 399, 400, 43, 401, 1, 397, 403, 404, 405, 406, 407, 408, 23, 409, 331, 1, 398, 1, 1, 43, 413, 414, 84, 85, 415, 416, 406, 414, 417, 418, 419, 1, 1, 43, 1, 321, 423, 424, 16, 425, 1, 1, 1, 43, 429, 430, 245, 431, 432, 43, 433, 434, 435, 1, 43, 437, 438, 439, 440, 441, 1, 1, 43, 72, 444, 445, 446, 447, 448, 449, 450, 451, 448, 452, 235, 274, 1, 1, 43, 455, 456, 148, 301, 457, 415, 458, 287, 284, 459, 416, 460, 461, 462, 463, 464, 465, 466, 1, 468, 469, 153]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Encoded example {i}:\", df['encoded'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ffd982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing token: welll\n",
      "Missing token: expectwhat\n",
      "Missing token: alansandoval\n",
      "Missing token: koren\n",
      "Missing token: pollitt\n",
      "Missing token: korencarpenter\n",
      "Missing token: pictwittercomfpaekypa\n",
      "Missing token: presidentialhow\n",
      "Missing token: goodine\n",
      "Missing token: sgoodine\n",
      "Missing token: schulze\n",
      "Missing token: thbthttt\n",
      "Missing token: wendywhistles\n",
      "Missing token: olderphoto\n"
     ]
    }
   ],
   "source": [
    "for token in df['tokens'].iloc[0]:\n",
    "    if token not in vocab:\n",
    "        print(\"Missing token:\", token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130c73e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m words_to_check \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwelll\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpectwhat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malansandoval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkoren\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpollitt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkorencarpenter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpictwittercomfpaekypa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresidentialhow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoodine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgoodine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschulze\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthbthttt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwendywhistles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molderphoto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words_to_check:\n\u001b[1;32m----> 8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(word)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "words_to_check = [\n",
    "    \"welll\", \"expectwhat\", \"alansandoval\", \"koren\", \"pollitt\", \n",
    "    \"korencarpenter\", \"pictwittercomfpaekypa\", \"presidentialhow\",\n",
    "    \"goodine\", \"sgoodine\", \"schulze\", \"thbthttt\", \"wendywhistles\", \"olderphoto\"\n",
    "]\n",
    "\n",
    "for word in words_to_check:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"Word: {word} -> Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd146d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnd-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
